# -*- coding: utf-8 -*-
"""waste_classification_with_keras_cv.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ggahytgBRJifhBEo6K9mnYn2d-X64d64
"""

!pip install -q --upgrade keras-cv
!pip install -q --upgrade keras  # Upgrade to Keras 3.

import os

os.environ["KERAS_BACKEND"] = "jax"  # @param ["tensorflow", "jax", "torch"]

import json
import math
import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf
import keras
from keras import losses
from keras import ops
from keras import optimizers
from keras.optimizers import schedules
from keras import metrics

import keras_cv

# Import tensorflow for `tf.data` and its preprocessing functions
import tensorflow as tf
import tensorflow_datasets as tfds

from google.colab import drive
drive.mount('/content/drive')

#dir_path = "/content/drive/MyDrive/Cases/Original Image Datasets/Animal"
dir_path = "/content/drive/MyDrive/garbage Image/TrashType_Image_Dataset"

BATCH_SIZE = 4
IMAGE_SIZE = (224, 224)
AUTOTUNE = tf.data.AUTOTUNE
tfds.disable_progress_bar()
train_generator = tf.keras.utils.image_dataset_from_directory(
    directory=dir_path,  color_mode='rgb',label_mode='int',
    image_size=IMAGE_SIZE,batch_size=BATCH_SIZE, shuffle=True,subset='training',validation_split=0.2,
    seed=25)
validation_generator = tf.keras.utils.image_dataset_from_directory(
    directory=dir_path, color_mode='rgb',
    label_mode='int',image_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,shuffle=False,subset='validation',validation_split=0.2,
    seed=25)

class_names = train_generator.class_names
print(class_names)

len(class_names)

train_ds_shuff = train_generator.shuffle(
    3*BATCH_SIZE, reshuffle_each_iteration=True
)
images = next(iter(train_ds_shuff.take(1)))[0]
images.shape

keras_cv.visualization.plot_image_gallery(images, value_range=(0, 255))

keras.backend.clear_session()

normalization_layer = tf.keras.layers.Rescaling(1./255)
normalized_train_ds = train_generator.map(lambda x, y: (normalization_layer(x), y))
normalized_test_ds = validation_generator.map(lambda x, y: (normalization_layer(x), y))

model = keras_cv.models.ImageClassifier.from_preset(
    "efficientnetv2_b0_imagenet", num_classes=len(class_names)
)

from tensorflow.keras.optimizers import RMSprop, Adam, AdamW
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=15,
                        verbose=2, restore_best_weights=True)
model.compile(optimizer=AdamW(learning_rate=0.001),loss='sparse_categorical_crossentropy',
              metrics = ['acc'])

model.summary()

r = model.fit(normalized_train_ds,
              validation_data=normalized_test_ds, callbacks=[monitor],
              epochs=15) #time taken: approximately 60m

model.save('cnn_good_classfier11.keras')

plt.plot(r.history['loss'], label='loss')
plt.plot(r.history['val_loss'], label='val_loss')
plt.legend()
plt.show()

from PIL import Image
import numpy as np

# Load the image
path = "/content/image.jpg"
image = Image.open(path)

# Convert the image to a numpy array
image = np.array(image)

# Normalize the image
image = image / 255.0

# Add a batch dimension
image = np.expand_dims(image, axis=0)

resizing = keras_cv.layers.Resizing(
    IMAGE_SIZE[0], IMAGE_SIZE[1], crop_to_aspect_ratio=True
)

np_im_rs = resizing(image)
np_im_rs.shape

"""Let's look at how our model performs after the fine tuning:"""

predictions = model.predict(np_im_rs)
predictions

print("Top class is:", class_names[np.argmax(predictions[0])])

import pickle

# Save the model to a pickle file
with open('model_transferL.pkl', 'wb') as file:
    pickle.dump(model, file)

print("Model saved successfully!")
